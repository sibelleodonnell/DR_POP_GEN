# ----------------- Installing angsd: refer to https://github.com/ANGSD/angsd
# This project used ANGSD version 0.932
# "FUZZY genotyping" with ANGSD - without calling actual genotypes but working with genotype likelihoods at each SNP. Optimal for low-coverage data (<10x).

# listing all bam filenames 
ls *bam >bams

module load htslib
module load zlib
module load bzip2
module load curl
module load openblas/0.3.20
module load r

# ----------------- STEP 1: Quality Filtering and Qrank output file
# angsd settings:
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -maxDepth 520"
TODO="-doQsDist 1 -doDepth 1 -doCounts 1 -dumpCounts 2"

#Chromosome - change for correct species. Find in the header of .sam files
# Past CHR="000000F_1"
# Ofav CHR="NW_018148670.1"
# Agar has no chromosome header (since we mapped to a denovo genome)

CHR="NW_018148670.1"

angsd -b bams -r $CHR -GL 1 $FILTERS $TODO -P 1 -out Sp

Rscript /project/path/to/plotQC.R prefix=dd bams=bams > qranks

#Remove low quality samples based on qranks file (For DR project, samples <.10) and ddpdf plots (fraction of sites v sample depth, fraction of sites v genotyping rate cutoff)

# ----------------- STEP 2: Initial ibsMat
# Set maxDepth to 10x the number of samples in your bams file
# Set minInd to 80% the number of samples in your bams file

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -maxDepth 800 -dosnpstat 1 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 8 -snp_pval 1e-5 -minMaf 0.05 -geno_minDepth 2"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out SppName_out_whole

### sftp bams file and .ibsMat file to Desktop, plot dendrograms in R following ANGSD_ibs.R in repo ###

# ----------------- STEP 2.5: FOR AGAR ONLY (removing linked loci by setting sites filter in ANGSD)
#Can just use AgarC_final_site.txt, or see below for how to remove linked loci
# Removing linked loci for Agaricia by -sites setting rather than ngsLD due mapping to a denovo reference 
# Performed prior to clonal removal due to lack of technical replicates for confident clonal verification 
# Method is used to tell ANGSD to only consider tags with one site on them when creating analyzing population structure

gzip -d Agar_out_whole.geno.gz
awk '{print $1}' Agar_out_whole.geno > tag.txt
awk '{print $2}' Agar_out_whole.geno > sites.txt
paste tag.txt sites.txt > tag_site.txt

# sftp out tag_site.txt and remove Tags with more than one Site

sort -u -o AgarC_final_site.txt unique_tag_site

# ANGSD settings same as STEP 2 plus the additional -sites filter

FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -maxDepth 800 -dosnpstat 1 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 8 -snp_pval 1e-5 -minMaf 0.05 -geno_minDepth 2 -sites AgarC_final_site.txt"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"


#Be sure your file has no header and remove quotations around tag names. Then index file. 
sed -i 's/"//g' AgarC_final_site.txt
angsd sites index AgarC_final_site.txt

angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out Agar_Final_out_whole

# ----------------- STEP 3: Pruning clones

# sftp bams and ibsMat files to Desktop and continue to DR_ibs.R
# Prune clones based on depth of technical replicates in the ibs dendrogram. For species with high clonality (Porites) or no technical replicates (Agaricia), repeat this step by removing all but one sample from each clonal cluster (retain the sample with the highest qrank value for each clonal group), creating a new, pruned bams file, and rerunning ANGSD (STEP2). Keep all technical replicates in your new bams file. Return to R and plot ibs dendrogram to inspect for more clones. 

#Rerun angsd on pruned bams file with same filters as STEP 2
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -maxDepth 800 -dosnpstat 1 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 8 -snp_pval 1e-5 -minMaf 0.05 -geno_minDepth 2"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

angsd -b bams -GL 1 $FILTERS $TODO -P 1 -out SppName_noclones_out_whole

#sftp bam and .ibsMat file to comuputer
#Plot resulting .ibsMat in order to identify and prune clones. Once you have only unique genotypes remaining, this is your final sample set for downstream population analyses.

# ----------------- STEP 4: FOR PAST AND OFAV ONLY (ngsLD)

# ngsLD - A program to estimate pairwise linkage disequilibrium
# Fox EA, Wright AE, Fumagalli M, and Vieira FG ngsLD: evaluating linkage disequilibrium using genotype likelihoods. Bioinformatics (2019) 35(19):3855 - 3856
# Used to remove linked loci after no clones remain in the dataset (post dendrogram), but before doing any downstream population analyses (pre admixture and Fst)
### Follow steps to install ngsLD: https://github.com/fgvieira/ngsLD

module list
module load gcc/8.3.0
module load intel/18.0.4
module load gsl/2.5
module list
module load conda
module load rust

#Check ngsLD is working run 'ngsLD'
ngsLD

#zcat geno.gz file of clonal pruned final dataset post dendrograms to get number of sites
zcat Spp_final_noclones_out_whole.geno.gz | head

#save as number of sites
NS=`zcat Spp_final_noclones_out_whole.geno.gz | wc -l`
echo $NS

#sites file for --pos filter in ngsLD
zcat Spp_final_noclones_out_whole.mafs.gz | tail -n +2 | cut -f1,2 > sites
head sites 

#running ngsLD to prune linked loci
#change n_ind to appropriate value
ngsLD --geno Spp_final_noclones_out_whole.geno.gz --n_ind 62 --n_sites $NS --pos sites --out Spp_ngsLD
head Spp_ngsLD 

#### now prune linked sites using _ngsLD file
###I made a conda environment but then learned the new ngsLD prune command uses Rust - see below for installing correct version of Rust
#### conda activate ngs

#Have to load 2018 version of rust to work for prune_graph installation. Only have to run this once: 
git clone https://github.com/fgvieira/prune_graph.git ###for prune_graph###
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh ###for rust###
rustup install 1.31.0 ###specific rust version###
cargo build --release
cargo test
###^^Be sure to do this in the created prune_graph directory after running GitHub installation

#Sftp out the _ngsLD file, open in excel and replace any #NAME values with NAN
#^ For DR_POP_GEN project: for values with NAN but a distance >50,000, keep those site pairs. Remove all other NAN value site pairs
# Save as a ngsLD.txt file (you can keep the header) and sftp .txt file back into the terminal. 
# put file back into working directory

##With preset prune_graph filters
# ./target/release/prune_graph --in Ofav_ngsLD.txt --out test.out
##

#####prune_graph filter settings from ngsLD GitHub example. Changed r2 to .1
/scratch1/seodonne/prune_graph/target/release/prune_graph --header --in Spp_ngsLD.txt --weight-field "r2" --weight-filter "dist <= 50000 && r2 >= 0.1" --out SppLD_unlinked.pos
#Get both chromosome and position for the angsd site setting 
sed 's/:/\t/g' SppLD_unlinked.pos > SppLD_unlinked.sites
sort SppLD_unlinked.sites >sorted.SppLD_unlinked.sites
#####

#module load all necessary angsd packages and run:
angsd sites index sorted.SppLD_unlinked.sites


######### Setting sites using unlinked file directly in ANGSD run.#########
#Set minInd to 50% number of samples, maxDepth to 10X number of samples
FILTERS="-uniqueOnly 1 -remove_bads 1 -minMapQ 20 -minQ 25 -maxDepth 620 -dosnpstat 1 -doHWE 1 -sb_pval 1e-5 -hetbias_pval 1e-5 -skipTriallelic 1 -minInd 31 -snp_pval 1e-5 -minMaf 0.05"

TODO="-doMajorMinor 1 -doMaf 1 -doCounts 1 -makeMatrix 1 -doIBS 1 -doCov 1 -doGeno 8 -doBcf 1 -doPost 1 -doGlf 2"

angsd -sites sorted.SppLD_unlinked.sites -b bams_final -GL 1 $FILTERS $TODO -P 1 -out Spp_final_LD

# record sites remaining
######### The files output from this run are the ones which will be used in Admixture and Fst steps #########

# ----------------- STEP 5: Admixture (NGS Admix and ADMIXTURE)
###NgsAdmix###
# for K from 2 to 6 : run each k 10 times - ngsAdmix is limited to calling a minimum of 2 populations - see ADMIXTURE below to determine if K=1
#this for loop will spit out a list of commands - paste them into a shell script (ex name: NGSadmix.sh) and run as a job
for j in `seq 1 20` ;
do
    for i in `seq 1 10` ; 
    do
        echo NGSadmix -likes Past_final_LD.beagle.gz -minMaf 0.05 -P 10 -K ${i} -o Past_final_LD_k${i}run${j}
    done
done

chmod +x *log
chmod +x *qopt
#sftp all log and qopt files onto computer in a separate admixture folder for each species - used in DR_ibs_pcoa_admix.R ADMIXTURE step
#sftp bam file as well for plotting in R

###ADMIXTURE###
module load openblas/0.3.20 
module load plink2
plink --vcf Spp_final_LD.bcf --make-bed --allow-extra-chr --double-id --out ADMIX_res20

# ADMIXTURE does not accept chromosome names that are not human chromosomes. We will thus just exchange the first column by 0
awk '{$1="0"; print $0}' ADMIX_res20.bim > ADMIX_res20.bim.tmp
cp ADMIX_res20.bim.tmp ADMIX_res20.bim
for j in `seq 1 10` ;
do
	for K in `seq 1 6`;
	do
echo admixture --cv ADMIX_res20.bed $K \| tee ADMIX_res20_${K}run${j}.out
	done
done
# paste output into shell script (ex name:ADMIX.sh) and run as job
# choose K based on least CV error
grep -h CV admixreal.out > PastLDchooseK.txt

#Once K is chosen based on C-val err (PastLDchooseK.txt), pull out the correct K run .Q file
cp ADMIX_res20.1.Q Past_K1.txt
chmod +x Past_K1.txt

# ----------------- STEP 6: Calculating global pairwise Fst using ANGSD
#####Input into a shell script (global_fst.sh)#####

#!/bin/bash
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=4GB
#SBATCH --time=00:45:00
#SBATCH --error=globalfst.err
#SBATCH --output=globalfst.out


# Define variables
REF_GENOME="/project/ckenkel_26/RefSeqs/OfavGenome_New/Ofav_v1.0_symABCD_longref.fasta"
POP1_BAM_LIST=EP
POP2_BAM_LIST=FM
POP3_BAM_LIST=PL
POP4_BAM_LIST=PA
POP5_BAM_LIST=CR
OUT_DIR="."
SITES_FILE=sorted.OfavLD_unlinked.sites

# Create output directories
mkdir -p ${OUT_DIR}/pop1
mkdir -p ${OUT_DIR}/pop2
mkdir -p ${OUT_DIR}/pop3
mkdir -p ${OUT_DIR}/pop4
mkdir -p ${OUT_DIR}/pop5
mkdir -p ${OUT_DIR}/fst

# Step 1: Calculate the SAF for each population
angsd -b ${POP1_BAM_LIST} -anc ${REF_GENOME} -out ${OUT_DIR}/pop1/pop1 -dosaf 1 -gl 1 -sites ${SITES_FILE}
angsd -b ${POP2_BAM_LIST} -anc ${REF_GENOME} -out ${OUT_DIR}/pop2/pop2 -dosaf 1 -gl 1 -sites ${SITES_FILE}
angsd -b ${POP3_BAM_LIST} -anc ${REF_GENOME} -out ${OUT_DIR}/pop3/pop3 -dosaf 1 -gl 1 -sites ${SITES_FILE}
angsd -b ${POP4_BAM_LIST} -anc ${REF_GENOME} -out ${OUT_DIR}/pop4/pop4 -dosaf 1 -gl 1 -sites ${SITES_FILE}
angsd -b ${POP5_BAM_LIST} -anc ${REF_GENOME} -out ${OUT_DIR}/pop5/pop5 -dosaf 1 -gl 1 -sites ${SITES_FILE}

# Step 2: Estimate the 2D-SFS prior
pop_pairs=( "pop1_pop2" "pop1_pop3" "pop1_pop4" "pop1_pop5" 
            "pop2_pop3" "pop2_pop4" "pop2_pop5"
            "pop3_pop4" "pop3_pop5"
            "pop4_pop5" )

for pair in "${pop_pairs[@]}"
do
  pop1=$(echo $pair | cut -d'_' -f1)
  pop2=$(echo $pair | cut -d'_' -f2)
  
  realSFS ${OUT_DIR}/${pop1}/${pop1}.saf.idx ${OUT_DIR}/${pop2}/${pop2}.saf.idx -P 4 > ${OUT_DIR}/fst/${pair}.sfs
done

# Step 3: Calculate the Fst index
for pair in "${pop_pairs[@]}"
do
  pop1=$(echo $pair | cut -d'_' -f1)
  pop2=$(echo $pair | cut -d'_' -f2)
  
  realSFS fst index ${OUT_DIR}/${pop1}/${pop1}.saf.idx ${OUT_DIR}/${pop2}/${pop2}.saf.idx -sfs ${OUT_DIR}/fst/${pair}.sfs -fstout ${OUT_DIR}/fst/${pair}
done


# Step 4: Get the global Fst estimate
for pair in "${pop_pairs[@]}"
do
  realSFS fst stats ${OUT_DIR}/fst/${pair}.fst.idx > ${OUT_DIR}/fst/${pair}_global_fst.txt
done
#####
sbatch global_fst.sh




# ----------------- STEP 6.5: Bootstrapping 95% confidence intervals for global Fst values
#####Input into a shell script (bootstrapping.sh)#####

#!/bin/bash
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=8GB
#SBATCH --time=00:45:00
#SBATCH --error=boot.err
#SBATCH --output=boot.out

POP1_IDX=/scratch1/seodonne/DR_bams/Ofav_bams/pop1/pop1.saf.idx 
POP2_IDX=/scratch1/seodonne/DR_bams/Ofav_bams/pop2/pop2.saf.idx 
POP3_IDK=/scratch1/seodonne/DR_bams/Ofav_bams/pop3/pop3.saf.idx 
POP4_IDX=/scratch1/seodonne/DR_bams/Ofav_bams/pop4/pop4.saf.idx 
POP5_IDX=/scratch1/seodonne/DR_bams/Ofav_bams/pop5/pop5.saf.idx 


#Bootstrap for each population pair
realSFS -bootstrap 100 ${POP1_IDX} ${POP2_IDX} > pop1pop2.sfs

realSFS -bootstrap 100 ${POP1_IDX} ${POP3_IDX} > pop1pop3.sfs

realSFS -bootstrap 100 ${POP1_IDX} ${POP4_IDX} > pop1pop4.sfs

realSFS -bootstrap 100 ${POP1_IDX} ${POP5_IDX} > pop1pop5.sfs

realSFS -bootstrap 100 ${POP2_IDX} ${POP3_IDX} > pop2pop3.sfs

realSFS -bootstrap 100 ${POP2_IDX} ${POP4_IDX} > pop2pop4.sfs

realSFS -bootstrap 100 ${POP2_IDX} ${POP5_IDX} > pop2pop5.sfs

realSFS -bootstrap 100 ${POP3_IDX} ${POP4_IDX} > pop3pop4.sfs

realSFS -bootstrap 100 ${POP3_IDX} ${POP5_IDX} > pop3pop5.sfs

realSFS -bootstrap 100 ${POP4_IDX} ${POP5_IDX} > pop4pop5.sfs


##### Chmod +x and run job #####

#Move resulting *sfs files to bootstraps directory (if not already there)
mv *sfs bootstraps
cd bootstraps
#Each sfs file contains a line per bootstrap run (100 lines each)

##### Input into shell script (fstboot.sh) #####
#!/bin/bash
#SBATCH --cpus-per-task=4
#SBATCH --mem-per-cpu=8GB
#SBATCH --time=00:45:00
#SBATCH --error=foot.err
#SBATCH --output=foot.out

# Define the list of population pairs and the number of bootstraps
pop_pairs=("pop1pop2" "pop1pop4" "pop2pop3" "pop2pop5" "pop3pop5" "pop1pop3" "pop1pop5" "pop2pop4" "pop3pop4" "pop4pop5")
num_bootstraps=100

# Define the base path to the SAF files (change this to match your directory structure)
base_saf_path="/scratch1/seodonne/DR_bams/Agar_bams"

# Loop through each population pair
for pair in "${pop_pairs[@]}"; do
  # Extract the individual population names from the pair
  pop1=$(echo $pair | grep -oP '(?<=pop)\d+' | head -n 1)
  pop2=$(echo $pair | grep -oP '(?<=pop)\d+' | tail -n 1)

  # Construct the paths to the SAF files for each population
  saf_path_pop1="${base_saf_path}/pop${pop1}/pop${pop1}.saf.idx"
  saf_path_pop2="${base_saf_path}/pop${pop2}/pop${pop2}.saf.idx"

  # Loop through each bootstrap iteration
  for ((i=1; i<=$num_bootstraps; i++)); do
    # Extract the SFS values for the current bootstrap iteration
    sfs_line=$(sed -n "${i}p" ${pair}.sfs)

    # Write the SFS values to a temporary file
    sfs_tmp_file=$(mktemp)
    echo $sfs_line > $sfs_tmp_file

    # Run the FST index command
    realSFS fst index ${saf_path_pop1} ${saf_path_pop2} -sfs $sfs_tmp_file -fstout ${pair}_boot${i}.fst.idx

    # Remove the temporary file
    rm $sfs_tmp_file

    # Extract FST values
    realSFS fst stats ${pair}_boot${i}.fst.idx.fst.idx > ${pair}_boot${i}_fst.txt
  done
done

##### chmod +x and run #####

#Combining all fst results into one final file for calculating confidence intervals in R

##### combfst.sh #####

#!/bin/bash
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=4GB
#SBATCH --time=00:45:00
#SBATCH --error=comb.err
#SBATCH --output=comb.out

# Define the list of population pairs and the number of bootstraps
pop_pairs=("pop1pop2" "pop1pop4" "pop2pop3" "pop2pop5" "pop3pop5" "pop1pop3" "pop1pop5" "pop2pop4" "pop3pop4" "pop4pop5")
num_bootstraps=100

# Define the output file
output_file="Agar_boot_fst_results.txt"

# Create or clear the output file
> $output_file

# Loop through each population pair
for pair in "${pop_pairs[@]}"; do
  # Loop through each bootstrap iteration
  for ((i=1; i<=$num_bootstraps; i++)); do
    # Construct the FST output file name
    fst_file="${pair}_boot${i}_fst.txt"

    # Check if the FST output file exists
    if [[ -f $fst_file ]]; then
      # Read the FST value from the file
      fst_value=$(awk '{print $1, $2}' $fst_file)

      # Append the population pair, bootstrap iteration, and FST value to the output file
      echo -e "${pair}_boot${i}\t${fst_value}" >> $output_file
    fi
  done
done

##### Run #####
sbatch combfst.sh
# sftp out final file and continue in Fst_95Intervals.R


# ----------------- STEP 7: Hard-call genotype pairwise Fst using vcftools
# to see sample names
module load bcftools
#make 5 list files (1 for each site) and have a list of all of the samples at that site. 
#File names should be CR, FM, EP, PL, PA
# Example: nano CR and then type out the list of all non-clonal samples at that site
module load vcftools
# first convert to vcf file
bcftools convert -O v -o Past_final_LD.vcf Past_final_LD.bcf

## paste this list into file called pops
PA
PL
FM
CR
EP
##

#Run lines below in the terminal, then paste outputs into a shell script to run as a job (ex name: Fst.sh)
for i in `cat ./pops`; do echo vcftools --vcf Past_final_LD.vcf --weir-fst-pop PA --weir-fst-pop $i --out Past_PA_LD_$i ; done
for i in `cat ./pops`; do echo vcftools --vcf Past_final_LD.vcf --weir-fst-pop PL --weir-fst-pop $i --out Past_PL_LD_$i ; done
for i in `cat ./pops`; do echo vcftools --vcf Past_final_LD.vcf --weir-fst-pop FM --weir-fst-pop $i --out Past_FM_LD_$i ; done
for i in `cat ./pops`; do echo vcftools --vcf Past_final_LD.vcf --weir-fst-pop CR --weir-fst-pop $i --out Past_CR_LD_$i ; done
for i in `cat ./pops`; do echo vcftools --vcf Past_final_LD.vcf --weir-fst-pop EP --weir-fst-pop $i --out Past_EP_LD_$i ; done

sbatch Fst.sh


#Collect pairwise Fst values into one file
grep "mean Fst" fst*.err | sed 's/Weir and Cockerham mean Fst estimate: //g' > Past_meanFst_LD.txt
grep "weighted Fst" fst*.err | sed 's/Weir and Cockerham weighted Fst estimate: //g' > Past_weightFst_LD.txt
grep "\--out" fst*.err | sed 's/\--out //g' | awk '$1=$1' > Past_PopPairs_LD.txt

paste Past_PopPairs_LD.txt Past_meanFst_LD.txt Past_weightFst_LD.txt > Past_FstFinal_LD.txt

#sftp Past_FstFinal_LD.txt onto computer for plotting in R

# ----------------- DONE: Continue moving through DR_ibs_pcoa_admix.R


